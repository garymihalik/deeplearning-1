{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lstm_utils\n",
    "import importlib\n",
    "importlib.reload(lstm_utils)\n",
    "from lstm_utils import *\n",
    "import sgdr\n",
    "importlib.reload(sgdr)\n",
    "from sgdr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(3)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = imdb.get_word_index()\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting pretrained word embeddings \n",
    "#The pickled files from fast.ai are not working. I pickled them again and worked\n",
    "#glove_path = get_glove_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = \"/data/yinterian/Glove/6B.50d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates an array of work embeddings for our IMDB datset. It is using Glove embedings. You can find more about Glove embeddings here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb(top_words, glove_path, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try recurrent_dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb], embeddings_regularizer=regularizers.l2(1e-5))(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 282s - loss: 0.6277 - acc: 0.7202 - val_loss: 0.4481 - val_acc: 0.8454\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 283s - loss: 0.4012 - acc: 0.8680 - val_loss: 0.4078 - val_acc: 0.8643\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 279s - loss: 0.3453 - acc: 0.8956 - val_loss: 0.4053 - val_acc: 0.8619\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 279s - loss: 0.3018 - acc: 0.9102 - val_loss: 0.3855 - val_acc: 0.8696\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 281s - loss: 0.2688 - acc: 0.9240 - val_loss: 0.3978 - val_acc: 0.8577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd90c051f28>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_79:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr.assign(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 283s - loss: 0.2491 - acc: 0.9292 - val_loss: 0.3987 - val_acc: 0.8772\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 283s - loss: 0.2191 - acc: 0.9431 - val_loss: 0.4002 - val_acc: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd90c051d68>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without pretrined embedings\n",
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 20, input_length=seq_len, mask_zero=True,\n",
    "              embeddings_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "x = Dropout(0.25)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model2 = Model(inputs=inputs, outputs=x)\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_159:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found this with LR_Finder\n",
    "model2.optimizer.lr.assign(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 467s - loss: 0.5486 - acc: 0.7142 - val_loss: 0.3861 - val_acc: 0.8429\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 466s - loss: 0.3373 - acc: 0.8696 - val_loss: 0.3280 - val_acc: 0.8704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd919fae5f8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_178:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.optimizer.lr.assign(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 464s - loss: 0.3181 - acc: 0.8810 - val_loss: 0.3461 - val_acc: 0.8672\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 467s - loss: 0.2924 - acc: 0.8935 - val_loss: 0.3524 - val_acc: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9cf8b5978>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 285s - loss: 18.6379 - acc: 0.5463 - val_loss: 235.2400 - val_acc: 0.4986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd91c619f98>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)\n",
    "cb = LR_Finder(steps_per_epoch, start_lr=1e-7, end_lr=10)\n",
    "model2.fit(trn, y_train, epochs=1, validation_data=(test, y_test), batch_size=batch_size, \n",
    "           callbacks=[cb], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9082385c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(cb.history['lr'], cb.losses)\n",
    "plt.ylim((0,2))\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "# weights=[emb]\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              embeddings_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "x = Dropout(0.4)(x)\n",
    "x = LSTM(70, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)\n",
    "def fc_cos(model, steps_per_epoch, nc,  cl, lr, tmult=1):\n",
    "    for i in range(nc):\n",
    "        cb = CosAnneal(steps_per_epoch, init_lr=lr)\n",
    "        model.fit(trn, y_train, validation_data=(test, y_test), epochs=cl,\n",
    "                         batch_size=64, callbacks=[cb])\n",
    "        cl *= tmult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 452s - loss: 0.5845 - acc: 0.7413 - val_loss: 0.4843 - val_acc: 0.8214\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 442s - loss: 0.5010 - acc: 0.8281 - val_loss: 0.4111 - val_acc: 0.8709\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 444s - loss: 0.4227 - acc: 0.8807 - val_loss: 0.3876 - val_acc: 0.8894\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.4145 - acc: 0.8938 - val_loss: 0.3989 - val_acc: 0.8924\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 445s - loss: 0.4132 - acc: 0.8997 - val_loss: 0.3980 - val_acc: 0.8949\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 446s - loss: 0.4092 - acc: 0.9032 - val_loss: 0.4075 - val_acc: 0.8927\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 444s - loss: 0.4053 - acc: 0.9070 - val_loss: 0.4077 - val_acc: 0.8946\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 455s - loss: 0.4064 - acc: 0.9082 - val_loss: 0.4233 - val_acc: 0.8916\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 446s - loss: 0.4083 - acc: 0.9112 - val_loss: 0.4277 - val_acc: 0.8928\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 455s - loss: 0.4129 - acc: 0.9107 - val_loss: 0.4287 - val_acc: 0.8933\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 445s - loss: 0.4099 - acc: 0.9150 - val_loss: 0.4297 - val_acc: 0.8966\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.4070 - acc: 0.9152 - val_loss: 0.4354 - val_acc: 0.8966\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 441s - loss: 0.4100 - acc: 0.9157 - val_loss: 0.4425 - val_acc: 0.8943\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.4145 - acc: 0.9158 - val_loss: 0.4479 - val_acc: 0.8958\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 447s - loss: 0.4050 - acc: 0.9195 - val_loss: 0.4585 - val_acc: 0.8960\n"
     ]
    }
   ],
   "source": [
    "# I found these parameters after some random search \n",
    "fc_cos(model, steps_per_epoch, nc=3, cl=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has many parameters. Here are some of them with their current defaul values\n",
    "* top_words (5000)\n",
    "* max_review_length (500)\n",
    "* Dropout value (0.2)\n",
    "* embeddings_regularizer (1e-5)\n",
    "* number of units in the LSTM (100)\n",
    "* size of the embeding (50)\n",
    "* start with a pre-trained embeding (we can use it for certain sizes)\n",
    "* optimization type (adam)\n",
    "* learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units_arr = np.array([16, 24, 64, 128, 256])\n",
    "embeding_size_arr = np.array([16, 24, 64, 128])\n",
    "emb_reg_arr = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "dropout_arr = np.array([0.0, 0.1, 0.2, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "units_arr = np.array([20, 30, 40, 50, 60, 70])\n",
    "embeding_size_arr = np.array([20, 30, 40, 50, 60, 70])\n",
    "emb_reg_arr = np.array([1e-2, 1e-3, 1e-4, 1e-5])\n",
    "dropout_arr = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "K = 20\n",
    "list_arr = [units_arr, embeding_size_arr, emb_reg_arr, dropout_arr]\n",
    "list_names = [\"units\", \"embeding_size\", \"emb_reg\", \"dropout\"]\n",
    "parm_list = {}\n",
    "for l, name in zip(list_arr, list_names):\n",
    "    parm_list[name] = random.choices(l, k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30 70 0.001 0.2\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 436s - loss: 0.6811 - acc: 0.7293 - val_loss: 0.5626 - val_acc: 0.8016\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 441s - loss: 0.6287 - acc: 0.8109 - val_loss: 0.4782 - val_acc: 0.8592\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 451s - loss: 0.6369 - acc: 0.8416 - val_loss: 0.4680 - val_acc: 0.8765\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 444s - loss: 0.6651 - acc: 0.8481 - val_loss: 0.4527 - val_acc: 0.8832\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 439s - loss: 0.5997 - acc: 0.8668 - val_loss: 0.4476 - val_acc: 0.8906\n",
      "1 70 50 0.0001 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 451s - loss: 0.5954 - acc: 0.7314 - val_loss: 0.5208 - val_acc: 0.7902\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 475s - loss: 0.4755 - acc: 0.8332 - val_loss: 0.3842 - val_acc: 0.8766\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 450s - loss: 0.3819 - acc: 0.8871 - val_loss: 0.3469 - val_acc: 0.8913\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 473s - loss: 0.3622 - acc: 0.8989 - val_loss: 0.3533 - val_acc: 0.8916\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 470s - loss: 0.3513 - acc: 0.9055 - val_loss: 0.3586 - val_acc: 0.8934\n",
      "2 40 60 0.0001 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 443s - loss: 0.5557 - acc: 0.7709 - val_loss: 0.4448 - val_acc: 0.8438\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 451s - loss: 0.5049 - acc: 0.8324 - val_loss: 0.4403 - val_acc: 0.8548\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 455s - loss: 0.5119 - acc: 0.8412 - val_loss: 0.4313 - val_acc: 0.8736\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 468s - loss: 0.4119 - acc: 0.8861 - val_loss: 0.3778 - val_acc: 0.8891\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 471s - loss: 0.3812 - acc: 0.9007 - val_loss: 0.3732 - val_acc: 0.8921\n",
      "3 20 60 1e-05 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 422s - loss: 0.4818 - acc: 0.7847 - val_loss: 0.3832 - val_acc: 0.8571\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 435s - loss: 0.4963 - acc: 0.7908 - val_loss: 0.5945 - val_acc: 0.7166\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 421s - loss: 0.4110 - acc: 0.8552 - val_loss: 0.4181 - val_acc: 0.8605\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 402s - loss: 0.3659 - acc: 0.8853 - val_loss: 0.3941 - val_acc: 0.8710\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 518s - loss: 0.3513 - acc: 0.8961 - val_loss: 0.4005 - val_acc: 0.8720\n",
      "4 20 60 0.0001 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.5542 - acc: 0.7674 - val_loss: 0.4388 - val_acc: 0.8495\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 486s - loss: 0.5890 - acc: 0.7416 - val_loss: 0.4369 - val_acc: 0.8607\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 468s - loss: 0.4432 - acc: 0.8718 - val_loss: 0.3936 - val_acc: 0.8830\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 479s - loss: 0.3990 - acc: 0.8920 - val_loss: 0.3833 - val_acc: 0.8887\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.3884 - acc: 0.9005 - val_loss: 0.3885 - val_acc: 0.8897\n",
      "5 20 40 0.0001 0.2\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 402s - loss: 0.5669 - acc: 0.7509 - val_loss: 0.4717 - val_acc: 0.8194\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 460s - loss: 0.5492 - acc: 0.7836 - val_loss: 0.4554 - val_acc: 0.8488\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 427s - loss: 0.4467 - acc: 0.8629 - val_loss: 0.4000 - val_acc: 0.8738\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 430s - loss: 0.3959 - acc: 0.8867 - val_loss: 0.3733 - val_acc: 0.8880\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 442s - loss: 0.3624 - acc: 0.8989 - val_loss: 0.3555 - val_acc: 0.8958\n",
      "6 60 70 1e-05 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 499s - loss: 0.5037 - acc: 0.7669 - val_loss: 0.3971 - val_acc: 0.8552\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 499s - loss: 0.3527 - acc: 0.8762 - val_loss: 0.3397 - val_acc: 0.8846\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 474s - loss: 0.3077 - acc: 0.9050 - val_loss: 0.3426 - val_acc: 0.8884\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 456s - loss: 0.2909 - acc: 0.9165 - val_loss: 0.3580 - val_acc: 0.8904\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 500s - loss: 0.2741 - acc: 0.9270 - val_loss: 0.3745 - val_acc: 0.8886\n",
      "7 40 30 0.0001 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 446s - loss: 0.5314 - acc: 0.7752 - val_loss: 0.4184 - val_acc: 0.8499\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 445s - loss: 0.5191 - acc: 0.8092 - val_loss: 0.4401 - val_acc: 0.8498\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 447s - loss: 0.5483 - acc: 0.7896 - val_loss: 0.4245 - val_acc: 0.8640\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 447s - loss: 0.4461 - acc: 0.8704 - val_loss: 0.4098 - val_acc: 0.8815\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.4475 - acc: 0.8761 - val_loss: 0.4321 - val_acc: 0.8759\n",
      "8 70 30 0.01 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 449s - loss: 0.7569 - acc: 0.6355 - val_loss: 0.6902 - val_acc: 0.6720\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 450s - loss: 0.8007 - acc: 0.6251 - val_loss: 0.6989 - val_acc: 0.6426\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 446s - loss: 0.8279 - acc: 0.7132 - val_loss: 0.6299 - val_acc: 0.7682\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.7638 - acc: 0.7219 - val_loss: 0.5408 - val_acc: 0.8136\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 449s - loss: 0.7764 - acc: 0.7714 - val_loss: 0.5315 - val_acc: 0.8513\n",
      "9 70 50 0.0001 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.6184 - acc: 0.7132 - val_loss: 0.4755 - val_acc: 0.8354\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 447s - loss: 0.4875 - acc: 0.8508 - val_loss: 0.4148 - val_acc: 0.8774\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.4430 - acc: 0.8807 - val_loss: 0.4057 - val_acc: 0.8851\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 444s - loss: 0.4182 - acc: 0.8880 - val_loss: 0.3993 - val_acc: 0.8873\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 449s - loss: 0.4068 - acc: 0.8960 - val_loss: 0.3963 - val_acc: 0.8880\n",
      "10 40 70 0.0001 0.5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 465s - loss: 0.5690 - acc: 0.7706 - val_loss: 0.4463 - val_acc: 0.8592\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 463s - loss: 0.5238 - acc: 0.8385 - val_loss: 0.4293 - val_acc: 0.8808\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 466s - loss: 0.4500 - acc: 0.8843 - val_loss: 0.4100 - val_acc: 0.8929\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 463s - loss: 0.4516 - acc: 0.8926 - val_loss: 0.4257 - val_acc: 0.8919\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 464s - loss: 0.4499 - acc: 0.8968 - val_loss: 0.4268 - val_acc: 0.8966\n",
      "11 20 50 0.0001 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.5843 - acc: 0.7517 - val_loss: 0.5236 - val_acc: 0.7992\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.5383 - acc: 0.8164 - val_loss: 0.4607 - val_acc: 0.8514\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 407s - loss: 0.5392 - acc: 0.8289 - val_loss: 0.4607 - val_acc: 0.8731\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 409s - loss: 0.4546 - acc: 0.8810 - val_loss: 0.4160 - val_acc: 0.8846\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.4246 - acc: 0.8905 - val_loss: 0.4091 - val_acc: 0.8886\n",
      "12 50 30 0.0001 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 461s - loss: 0.6649 - acc: 0.6588 - val_loss: 0.6430 - val_acc: 0.7036\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 468s - loss: 0.5267 - acc: 0.7971 - val_loss: 0.4412 - val_acc: 0.8510\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 464s - loss: 0.4891 - acc: 0.8410 - val_loss: 0.4419 - val_acc: 0.8614\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 465s - loss: 0.6511 - acc: 0.7302 - val_loss: 0.7018 - val_acc: 0.6688\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 465s - loss: 0.5099 - acc: 0.8272 - val_loss: 0.4364 - val_acc: 0.8684\n",
      "13 70 30 1e-05 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 450s - loss: 0.5502 - acc: 0.7252 - val_loss: 0.4256 - val_acc: 0.8228\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 452s - loss: 0.3910 - acc: 0.8470 - val_loss: 0.3478 - val_acc: 0.8707\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 449s - loss: 0.3130 - acc: 0.8888 - val_loss: 0.3200 - val_acc: 0.8860\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 448s - loss: 0.2795 - acc: 0.9079 - val_loss: 0.3204 - val_acc: 0.8903\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 451s - loss: 0.2500 - acc: 0.9218 - val_loss: 0.3182 - val_acc: 0.8930\n",
      "14 20 50 1e-05 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 409s - loss: 0.5483 - acc: 0.7403 - val_loss: 0.4932 - val_acc: 0.7876\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 406s - loss: 0.4519 - acc: 0.8130 - val_loss: 0.4013 - val_acc: 0.8483\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 409s - loss: 0.3705 - acc: 0.8657 - val_loss: 0.3702 - val_acc: 0.8678\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 408s - loss: 0.3478 - acc: 0.8826 - val_loss: 0.3833 - val_acc: 0.8655\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 404s - loss: 0.3206 - acc: 0.8972 - val_loss: 0.3824 - val_acc: 0.8728\n",
      "15 20 60 0.001 0.5\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.7499 - acc: 0.6806 - val_loss: 0.6301 - val_acc: 0.7531\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.7877 - acc: 0.7380 - val_loss: 0.6466 - val_acc: 0.8119\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 413s - loss: 0.7092 - acc: 0.8300 - val_loss: 0.4995 - val_acc: 0.8766\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 413s - loss: 0.6207 - acc: 0.8614 - val_loss: 0.4400 - val_acc: 0.8898\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 415s - loss: 0.5872 - acc: 0.8703 - val_loss: 0.4324 - val_acc: 0.8900\n",
      "16 30 50 0.0001 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 433s - loss: 0.5945 - acc: 0.7338 - val_loss: 0.4932 - val_acc: 0.8088\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 431s - loss: 0.5724 - acc: 0.7942 - val_loss: 0.5017 - val_acc: 0.8453\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 433s - loss: 0.5154 - acc: 0.8550 - val_loss: 0.4714 - val_acc: 0.8638\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 432s - loss: 0.4396 - acc: 0.8800 - val_loss: 0.4127 - val_acc: 0.8812\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 434s - loss: 0.4309 - acc: 0.8872 - val_loss: 0.4229 - val_acc: 0.8825\n",
      "17 20 60 0.0001 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.5975 - acc: 0.7372 - val_loss: 0.4852 - val_acc: 0.8158\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.7056 - acc: 0.6730 - val_loss: 0.6620 - val_acc: 0.7139\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.5256 - acc: 0.8224 - val_loss: 0.4266 - val_acc: 0.8685\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 413s - loss: 0.4216 - acc: 0.8746 - val_loss: 0.3951 - val_acc: 0.8785\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.4097 - acc: 0.8837 - val_loss: 0.3878 - val_acc: 0.8830\n",
      "18 50 40 0.001 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 472s - loss: 0.6786 - acc: 0.6960 - val_loss: 0.5369 - val_acc: 0.7978\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 472s - loss: 0.6753 - acc: 0.7683 - val_loss: 0.5256 - val_acc: 0.8532\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 474s - loss: 0.6097 - acc: 0.8373 - val_loss: 0.4548 - val_acc: 0.8810\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 471s - loss: 0.5656 - acc: 0.8687 - val_loss: 0.4397 - val_acc: 0.8863\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 474s - loss: 0.5565 - acc: 0.8723 - val_loss: 0.4313 - val_acc: 0.8911\n",
      "19 40 60 1e-05 0.2\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 456s - loss: 0.5265 - acc: 0.7531 - val_loss: 0.4198 - val_acc: 0.8350\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 455s - loss: 0.3870 - acc: 0.8473 - val_loss: 0.3374 - val_acc: 0.8748\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 453s - loss: 0.2967 - acc: 0.9028 - val_loss: 0.3195 - val_acc: 0.8903\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 456s - loss: 0.2687 - acc: 0.9191 - val_loss: 0.3347 - val_acc: 0.8897\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 454s - loss: 0.2503 - acc: 0.9292 - val_loss: 0.3526 - val_acc: 0.8887\n"
     ]
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    [u, emb, reg, d]  = [parm_list[name][i] for name in list_names]\n",
    "    print(i, u, emb, reg, d)\n",
    "    model = get_LSTM_model(units=u, embeding_size=emb, emb_reg=reg, dropout=d)\n",
    "    fc_cos(model, steps_per_epoch, nc=1, cl=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
