{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import lstm_utils\n",
    "import importlib\n",
    "importlib.reload(lstm_utils)\n",
    "from lstm_utils import *\n",
    "import sgdr\n",
    "importlib.reload(sgdr)\n",
    "from sgdr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(3)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = imdb.get_word_index()\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting pretrained word embeddings \n",
    "#The pickled files from fast.ai are not working. I pickled them again and worked\n",
    "#glove_path = get_glove_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = \"/data/yinterian/Glove/6B.50d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates an array of work embeddings for our IMDB datset. It is using Glove embedings. You can find more about Glove embeddings here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb(top_words, glove_path, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try recurrent_dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb], embeddings_regularizer=regularizers.l2(1e-5))(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 282s - loss: 0.6277 - acc: 0.7202 - val_loss: 0.4481 - val_acc: 0.8454\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 283s - loss: 0.4012 - acc: 0.8680 - val_loss: 0.4078 - val_acc: 0.8643\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 279s - loss: 0.3453 - acc: 0.8956 - val_loss: 0.4053 - val_acc: 0.8619\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 279s - loss: 0.3018 - acc: 0.9102 - val_loss: 0.3855 - val_acc: 0.8696\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 281s - loss: 0.2688 - acc: 0.9240 - val_loss: 0.3978 - val_acc: 0.8577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd90c051f28>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_79:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr.assign(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 283s - loss: 0.2491 - acc: 0.9292 - val_loss: 0.3987 - val_acc: 0.8772\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 283s - loss: 0.2191 - acc: 0.9431 - val_loss: 0.4002 - val_acc: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd90c051d68>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without pretrined embedings\n",
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 20, input_length=seq_len, mask_zero=True,\n",
    "              embeddings_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "x = Dropout(0.25)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model2 = Model(inputs=inputs, outputs=x)\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_159:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found this with LR_Finder\n",
    "model2.optimizer.lr.assign(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 467s - loss: 0.5486 - acc: 0.7142 - val_loss: 0.3861 - val_acc: 0.8429\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 466s - loss: 0.3373 - acc: 0.8696 - val_loss: 0.3280 - val_acc: 0.8704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd919fae5f8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_178:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.optimizer.lr.assign(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 464s - loss: 0.3181 - acc: 0.8810 - val_loss: 0.3461 - val_acc: 0.8672\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 467s - loss: 0.2924 - acc: 0.8935 - val_loss: 0.3524 - val_acc: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9cf8b5978>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 285s - loss: 18.6379 - acc: 0.5463 - val_loss: 235.2400 - val_acc: 0.4986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd91c619f98>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)\n",
    "cb = LR_Finder(steps_per_epoch, start_lr=1e-7, end_lr=10)\n",
    "model2.fit(trn, y_train, epochs=1, validation_data=(test, y_test), batch_size=batch_size, \n",
    "           callbacks=[cb], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9082385c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(cb.history['lr'], cb.losses)\n",
    "plt.ylim((0,2))\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb], embeddings_regularizer=regularizers.l2(1e-5))(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "steps_per_epoch = math.ceil(25000/batch_size)\n",
    "def fc_cos(model, steps_per_epoch, nc,  cl, lr, tmult=1):\n",
    "    for i in range(nc):\n",
    "        cb = CosAnneal(steps_per_epoch, init_lr=lr)\n",
    "        model.fit(trn, y_train, validation_data=(test, y_test), epochs=cl,\n",
    "                         batch_size=64, callbacks=[cb])\n",
    "        cl *= tmult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 463s - loss: 0.5282 - acc: 0.7648 - val_loss: 0.3989 - val_acc: 0.8488\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 465s - loss: 0.3745 - acc: 0.8672 - val_loss: 0.3577 - val_acc: 0.8766\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 464s - loss: 0.3230 - acc: 0.8972 - val_loss: 0.3537 - val_acc: 0.8830\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 467s - loss: 0.3076 - acc: 0.9092 - val_loss: 0.3609 - val_acc: 0.8847\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 466s - loss: 0.2872 - acc: 0.9198 - val_loss: 0.3692 - val_acc: 0.8890\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 463s - loss: 0.2638 - acc: 0.9317 - val_loss: 0.3779 - val_acc: 0.8863\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 464s - loss: 0.2418 - acc: 0.9397 - val_loss: 0.3866 - val_acc: 0.8865\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 466s - loss: 0.2286 - acc: 0.9473 - val_loss: 0.4049 - val_acc: 0.8862\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 466s - loss: 0.2300 - acc: 0.9484 - val_loss: 0.4204 - val_acc: 0.8840\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 467s - loss: 0.2192 - acc: 0.9542 - val_loss: 0.4361 - val_acc: 0.8836\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 465s - loss: 0.2132 - acc: 0.9568 - val_loss: 0.4483 - val_acc: 0.8869\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 463s - loss: 0.2030 - acc: 0.9609 - val_loss: 0.4563 - val_acc: 0.8838\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 462s - loss: 0.2168 - acc: 0.9579 - val_loss: 0.4651 - val_acc: 0.8831\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 462s - loss: 0.2085 - acc: 0.9617 - val_loss: 0.4807 - val_acc: 0.8831\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 461s - loss: 0.2025 - acc: 0.9656 - val_loss: 0.5047 - val_acc: 0.8813\n"
     ]
    }
   ],
   "source": [
    "# LSTM is a different beast, It keeps overfitting after lr restarts. \n",
    "fc_cos(model, steps_per_epoch, nc=3, cl=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has many parameters. Here are some of them with their current defaul values\n",
    "* top_words (5000)\n",
    "* max_review_length (500)\n",
    "* Dropout value (0.2)\n",
    "* embeddings_regularizer (1e-5)\n",
    "* number of units in the LSTM (100)\n",
    "* size of the embeding (50)\n",
    "* start with a pre-trained embeding (we can use it for certain sizes)\n",
    "* optimization type (adam)\n",
    "* learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units_arr = np.array([16, 24, 64, 128, 256])\n",
    "embeding_size_arr = np.array([16, 24, 64, 128])\n",
    "emb_reg_arr = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "dropout_arr = np.array([0.0, 0.1, 0.2, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "K = 50\n",
    "list_arr = [units_arr, embeding_size_arr, emb_reg_arr, dropout_arr]\n",
    "list_names = [\"units\", \"embeding_size\", \"emb_reg\", \"dropout\"]\n",
    "parm_list = {}\n",
    "for l, name in zip(list_arr, list_names):\n",
    "    parm_list[name] = random.choices(l, k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 24 0.1 0.2\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "[u, emb, reg, d]  = [parm_list[name][i] for name in list_names]\n",
    "print(u, emb, reg, d)\n",
    "model = get_LSTM_model(units=u, embeding_size=emb, emb_reg=reg, dropout=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 0.0001 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 373s - loss: 0.5449 - acc: 0.7540 - val_loss: 0.4557 - val_acc: 0.8271\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 372s - loss: 0.6700 - acc: 0.6638 - val_loss: 0.7265 - val_acc: 0.5821\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 371s - loss: 0.5224 - acc: 0.7854 - val_loss: 0.4221 - val_acc: 0.8597\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 372s - loss: 0.3895 - acc: 0.8788 - val_loss: 0.3791 - val_acc: 0.8772\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 372s - loss: 0.3427 - acc: 0.8965 - val_loss: 0.3594 - val_acc: 0.8838\n",
      "23 64 0.001 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 394s - loss: 0.7028 - acc: 0.6976 - val_loss: 0.5695 - val_acc: 0.7938\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 386s - loss: 0.7095 - acc: 0.7778 - val_loss: 0.5498 - val_acc: 0.8438\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 382s - loss: 0.5955 - acc: 0.8530 - val_loss: 0.4200 - val_acc: 0.8830\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 382s - loss: 0.5660 - acc: 0.8719 - val_loss: 0.4110 - val_acc: 0.8918\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 383s - loss: 0.5432 - acc: 0.8753 - val_loss: 0.4020 - val_acc: 0.8943\n",
      "23 128 0.01 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 396s - loss: 0.8661 - acc: 0.6688 - val_loss: 0.6253 - val_acc: 0.7900\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 396s - loss: 0.8769 - acc: 0.7435 - val_loss: 0.5738 - val_acc: 0.8329\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 395s - loss: 1.2696 - acc: 0.7160 - val_loss: 1.0011 - val_acc: 0.8147\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 398s - loss: 1.3683 - acc: 0.7146 - val_loss: 0.9838 - val_acc: 0.7205\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 396s - loss: 1.2949 - acc: 0.7466 - val_loss: 0.7640 - val_acc: 0.8255\n",
      "64 64 1e-05 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 415s - loss: 0.4892 - acc: 0.7735 - val_loss: 0.3420 - val_acc: 0.8666\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 416s - loss: 0.3407 - acc: 0.8779 - val_loss: 0.3195 - val_acc: 0.8894\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 416s - loss: 0.2871 - acc: 0.9113 - val_loss: 0.3263 - val_acc: 0.8938\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 417s - loss: 0.2790 - acc: 0.9208 - val_loss: 0.3508 - val_acc: 0.8930\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 417s - loss: 0.2637 - acc: 0.9308 - val_loss: 0.3659 - val_acc: 0.8913\n",
      "16 64 0.1 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 369s - loss: 1.0708 - acc: 0.5608 - val_loss: 0.6649 - val_acc: 0.6760\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 369s - loss: 0.8500 - acc: 0.5602 - val_loss: 0.7328 - val_acc: 0.5654\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 367s - loss: 0.9249 - acc: 0.6305 - val_loss: 0.6477 - val_acc: 0.7539\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 369s - loss: 0.9358 - acc: 0.5778 - val_loss: 0.7073 - val_acc: 0.5740\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 367s - loss: 0.9111 - acc: 0.5700 - val_loss: 0.6965 - val_acc: 0.5651\n",
      "64 24 0.0001 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.6064 - acc: 0.7044 - val_loss: 0.5305 - val_acc: 0.7659\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.5080 - acc: 0.7951 - val_loss: 0.3833 - val_acc: 0.8672\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.3673 - acc: 0.8842 - val_loss: 0.3429 - val_acc: 0.8888\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.3320 - acc: 0.9024 - val_loss: 0.3346 - val_acc: 0.8943\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.3171 - acc: 0.9098 - val_loss: 0.3388 - val_acc: 0.8942\n",
      "64 24 0.001 0.2\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 409s - loss: 0.6494 - acc: 0.7140 - val_loss: 0.5541 - val_acc: 0.7884\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.6457 - acc: 0.7466 - val_loss: 0.5048 - val_acc: 0.8380\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.5229 - acc: 0.8545 - val_loss: 0.3917 - val_acc: 0.8860\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.4621 - acc: 0.8744 - val_loss: 0.3676 - val_acc: 0.8903\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 411s - loss: 0.4453 - acc: 0.8829 - val_loss: 0.3672 - val_acc: 0.8956\n",
      "256 128 0.1 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 516s - loss: 1.4068 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 513s - loss: 0.6953 - acc: 0.4992 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 514s - loss: 0.6944 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 514s - loss: 0.6947 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 515s - loss: 0.6949 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4999\n",
      "16 24 0.1 0.4\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 366s - loss: 0.8267 - acc: 0.5103 - val_loss: 0.6932 - val_acc: 0.5369\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 366s - loss: 1.0494 - acc: 0.5076 - val_loss: 0.7826 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 366s - loss: 0.7188 - acc: 0.4989 - val_loss: 0.7051 - val_acc: 0.5019\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 373s - loss: 0.6997 - acc: 0.4970 - val_loss: 0.6959 - val_acc: 0.5002\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 368s - loss: 0.6977 - acc: 0.5136 - val_loss: 0.6937 - val_acc: 0.5497\n",
      "64 24 1e-06 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.5456 - acc: 0.7206 - val_loss: 0.4827 - val_acc: 0.7717\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 416s - loss: 0.3630 - acc: 0.8450 - val_loss: 0.3244 - val_acc: 0.8655\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 474s - loss: 0.2813 - acc: 0.8868 - val_loss: 0.2918 - val_acc: 0.8812\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 423s - loss: 0.2321 - acc: 0.9122 - val_loss: 0.2866 - val_acc: 0.8860\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 412s - loss: 0.2018 - acc: 0.9251 - val_loss: 0.2934 - val_acc: 0.8865\n",
      "256 128 0.001 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 517s - loss: 1.9284 - acc: 0.5184 - val_loss: 2.0940 - val_acc: 0.5356\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 517s - loss: 2.0368 - acc: 0.5304 - val_loss: 1.9963 - val_acc: 0.5408\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 516s - loss: 1.9710 - acc: 0.5364 - val_loss: 1.9417 - val_acc: 0.5403\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 517s - loss: 1.9339 - acc: 0.5391 - val_loss: 1.9018 - val_acc: 0.5556\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 518s - loss: 1.8993 - acc: 0.5510 - val_loss: 1.8699 - val_acc: 0.5578\n",
      "64 24 0.0001 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.5644 - acc: 0.7378 - val_loss: 0.4477 - val_acc: 0.8296\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.5033 - acc: 0.8100 - val_loss: 0.4385 - val_acc: 0.8496\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 409s - loss: 0.4944 - acc: 0.8123 - val_loss: 0.4067 - val_acc: 0.8642\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.4525 - acc: 0.8431 - val_loss: 0.3975 - val_acc: 0.8710\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 410s - loss: 0.3759 - acc: 0.8885 - val_loss: 0.3766 - val_acc: 0.8850\n",
      "64 128 0.01 0.1\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 419s - loss: 0.8437 - acc: 0.6155 - val_loss: 0.6550 - val_acc: 0.6622\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 419s - loss: 0.8565 - acc: 0.6586 - val_loss: 0.8046 - val_acc: 0.5966\n",
      "Epoch 3/5\n",
      "22656/25000 [==========================>...] - ETA: 26s - loss: 0.9469 - acc: 0.6640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-090e503f5860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_LSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfc_cos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-9dd36f2449fd>\u001b[0m in \u001b[0;36mfc_cos\u001b[0;34m(model, steps_per_epoch, nc, cl, lr, tmult)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCosAnneal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         model.fit(trn, y_train, validation_data=(test, y_test), epochs=cl,\n\u001b[0;32m----> 7\u001b[0;31m                          batch_size=64, callbacks=[cb])\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcl\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtmult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K = 20\n",
    "for i in range(K):\n",
    "    [u, emb, reg, d]  = [parm_list[name][i] for name in list_names]\n",
    "    print(u, emb, reg, d)\n",
    "    model = get_LSTM_model(units=u, embeding_size=emb, emb_reg=reg, dropout=d)\n",
    "    fc_cos(model, steps_per_epoch, nc=1, cl=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "units_arr = np.array([20, 30, 40, 50, 60, 70])\n",
    "embeding_size_arr = np.array([20, 30, 40, 50, 60, 70])\n",
    "emb_reg_arr = np.array([1e-2, 1e-3, 1e-4, 1e-5])\n",
    "dropout_arr = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "K = 20\n",
    "list_arr = [units_arr, embeding_size_arr, emb_reg_arr, dropout_arr]\n",
    "list_names = [\"units\", \"embeding_size\", \"emb_reg\", \"dropout\"]\n",
    "parm_list = {}\n",
    "for l, name in zip(list_arr, list_names):\n",
    "    parm_list[name] = random.choices(l, k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60 40 1e-05 0.3\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      " 5056/25000 [=====>........................] - ETA: 277s - loss: 0.6063 - acc: 0.6699"
     ]
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    [u, emb, reg, d]  = [parm_list[name][i] for name in list_names]\n",
    "    print(i, u, emb, reg, d)\n",
    "    model = get_LSTM_model(units=u, embeding_size=emb, emb_reg=reg, dropout=d)\n",
    "    fc_cos(model, steps_per_epoch, nc=1, cl=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
