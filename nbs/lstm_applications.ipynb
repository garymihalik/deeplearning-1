{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_utils\n",
    "import importlib\n",
    "importlib.reload(lstm_utils)\n",
    "from lstm_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(3)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_trn), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = imdb.get_word_index()\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting pretrained word embeddings \n",
    "#The pickled files from fast.ai are not working. I pickled them again and worked\n",
    "#glove_path = get_glove_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"/data/yinterian/Glove/6B.50d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates an array of work embeddings for our IMDB datset. It is using Glove embedings. You can find more about Glove embeddings here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = create_emb(top_words, glove_path, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(100, implementation=2)`\n"
     ]
    }
   ],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb], trainable=False),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 469s - loss: 0.6873 - acc: 0.5359 - val_loss: 0.6830 - val_acc: 0.5536\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 473s - loss: 0.6735 - acc: 0.5718 - val_loss: 0.6717 - val_acc: 0.5720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ea81a8b38>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/home/yinterian/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(5000, 50, input_length=500, mask_zero=True, weights=[array([[ ..., embeddings_regularizer=<keras.reg...)`\n",
      "/home/yinterian/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(100, implementation=2)`\n"
     ]
    }
   ],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "model2 = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb], W_regularizer=l2(1e-6), dropout=0.2),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 469s - loss: 0.5396 - acc: 0.7186 - val_loss: 0.3588 - val_acc: 0.8528\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 467s - loss: 0.4096 - acc: 0.8278 - val_loss: 0.3820 - val_acc: 0.8477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f3629ccf8>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_88:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.optimizer.lr.assign(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 453s - loss: 0.3136 - acc: 0.8782 - val_loss: 0.3306 - val_acc: 0.8679\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 471s - loss: 0.2854 - acc: 0.8910 - val_loss: 0.3206 - val_acc: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f37cadef0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try recurrent_dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = top_words\n",
    "seq_len = max_review_length\n",
    "\n",
    "inputs = Input(shape=(seq_len,), dtype='int32')\n",
    "x = Embedding(vocab_size, 50, input_length=seq_len, mask_zero=True,\n",
    "              weights=[emb])(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(100, implementation=2)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 461s - loss: 0.5654 - acc: 0.6898 - val_loss: 0.3925 - val_acc: 0.8375\n",
      "Epoch 2/2\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8529"
     ]
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
